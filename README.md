Student Score Prediction
This project, developed during my internship at Elevvo, predicts student exam scores using linear and polynomial regression models on the "Student Performance Factors" dataset from Kaggle. The goal is to analyze how factors like study hours, attendance, and parental involvement impact academic performance.
Project Overview

Objective: Predict Exam_Score using features like Hours_Studied, Attendance, Sleep_Hours, and more.
Dataset: Student Performance Factors from Kaggle.
Tools: Python, Pandas, Scikit-learn, Matplotlib, Seaborn.
Models:
Linear Regression (MSE: 4.87, R²: 0.69)
Polynomial Regression (Degree 3) (MSE: ~4.95, R²: ~0.68, subject to variation)


Improvements: Feature scaling, outlier removal, cross-validation, and feature importance analysis.

Repository Structure

student_score_prediction.py: Main script with data cleaning, visualization, and model training.
*.png: Output visualizations (e.g., correlation heatmap, residual plots, model comparison).

Setup and Installation

Clone the Repository:
git clone https://github.com/your-username/student-score-prediction.git
cd student-score-prediction

Install Dependencies:
pip install pandas numpy matplotlib seaborn scikit-learn

Dataset:

Download the dataset from Kaggle.
Update the file_path in student_score_prediction.py to point to your local StudentPerformanceFactors.csv.

Run the Script:
python student_score_prediction.py

Key Features

Data Cleaning: Removes missing values, outliers (IQR method), and ensures valid ranges.
Visualizations:
Numerical and categorical feature distributions.
Scatter plots, correlation heatmap, box plots, and train-test distribution.
Actual vs. predicted scores and residual plots for both models.
Model comparison (MSE and R²).

Modeling:
Linear regression with feature scaling and cross-validation.
Polynomial regression (degree 3) to capture non-linear patterns.
Feature importance analysis to identify key predictors.

Results
Linear Regression: MSE = 4.87, R² = 0.69
Polynomial Regression (Degree 3): Slightly higher MSE, indicating linear model may be sufficient.
Cross-validation ensures robust performance estimates.
Visualizations provide insights into data and model performance.

Visualizations
Example outputs (generated by the script):

correlation_heatmap.png: Shows feature correlations.
model_comparison.png: Compares MSE and R² of both models.
linear_regression_residuals.png: Analyzes prediction errors.

Future Improvements
Test additional polynomial degrees or interaction terms.
Explore feature selection based on feature importance.
Try alternative regression models (e.g., Ridge, Lasso) for better performance.

Acknowledgments
Developed during my internship at Elevvo, where I gained hands-on experience in data science and machine learning.
Dataset sourced from Kaggle.
